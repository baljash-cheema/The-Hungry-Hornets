{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Checkpoint5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKB8YaRk05Sl"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-research/tapas/blob/master/notebooks/sqa_predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5EACclxE7sP"
      },
      "source": [
        "Running a Tapas fine-tuned checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uI6zyIM20Kw4",
        "outputId": "9e610e2d-7ee5-4793-d5c8-04d9f84cc60e"
      },
      "source": [
        "! pip install tapas-table-parsing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tapas-table-parsing\n",
            "  Downloading tapas_table_parsing-0.0.1.dev0-py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting pandas~=1.0.0\n",
            "  Downloading pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 38.0 MB/s \n",
            "\u001b[?25hCollecting tensorflow~=2.2.0\n",
            "  Downloading tensorflow-2.2.3-cp37-cp37m-manylinux2010_x86_64.whl (516.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 516.4 MB 18 kB/s \n",
            "\u001b[?25hCollecting nltk~=3.5\n",
            "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 26.1 MB/s \n",
            "\u001b[?25hCollecting tensorflow-probability==0.10.1\n",
            "  Downloading tensorflow_probability-0.10.1-py2.py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 28.3 MB/s \n",
            "\u001b[?25hCollecting scikit-learn~=0.22.1\n",
            "  Downloading scikit_learn-0.22.2.post1-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 47.0 MB/s \n",
            "\u001b[?25hCollecting tf-slim~=1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 47.9 MB/s \n",
            "\u001b[?25hCollecting tf-models-official~=2.2.0\n",
            "  Downloading tf_models_official-2.2.2-py2.py3-none-any.whl (711 kB)\n",
            "\u001b[K     |████████████████████████████████| 711 kB 47.3 MB/s \n",
            "\u001b[?25hCollecting frozendict==1.2\n",
            "  Downloading frozendict-1.2.tar.gz (2.6 kB)\n",
            "Collecting kaggle<1.5.8\n",
            "  Downloading kaggle-1.5.6.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting apache-beam[gcp]==2.20.0\n",
            "  Downloading apache_beam-2.20.0-cp37-cp37m-manylinux1_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 37.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.19.5)\n",
            "Requirement already satisfied: future<1.0.0,>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (0.16.0)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (3.12.1)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.3.0)\n",
            "Collecting typing-extensions<3.8.0,>=3.7.0\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting httplib2<=0.12.0,>=0.8\n",
            "  Downloading httplib2-0.12.0.tar.gz (218 kB)\n",
            "\u001b[K     |████████████████████████████████| 218 kB 54.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.5.0.post1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (3.17.3)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (2018.9)\n",
            "Requirement already satisfied: grpcio<2,>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.42.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.7)\n",
            "Collecting oauth2client<4,>=2.0.1\n",
            "  Downloading oauth2client-3.0.0.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting pyarrow<0.17.0,>=0.15.1\n",
            "  Downloading pyarrow-0.16.0-cp37-cp37m-manylinux2014_x86_64.whl (63.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 63.1 MB 37 kB/s \n",
            "\u001b[?25hCollecting fastavro<0.22,>=0.21.4\n",
            "  Downloading fastavro-0.21.24-cp37-cp37m-manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 50.9 MB/s \n",
            "\u001b[?25hCollecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n",
            "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
            "Collecting mock<3.0.0,>=1.0.1\n",
            "  Downloading mock-2.0.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 52.2 MB/s \n",
            "\u001b[?25hCollecting grpcio-gcp<1,>=0.2.2\n",
            "  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
            "Collecting google-cloud-language<2,>=1.3.0\n",
            "  Downloading google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting google-apitools<0.5.29,>=0.5.28\n",
            "  Downloading google-apitools-0.5.28.tar.gz (172 kB)\n",
            "\u001b[K     |████████████████████████████████| 172 kB 52.0 MB/s \n",
            "\u001b[?25hCollecting google-cloud-spanner<1.14.0,>=1.13.0\n",
            "  Downloading google_cloud_spanner-1.13.0-py2.py3-none-any.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 50.4 MB/s \n",
            "\u001b[?25hCollecting google-cloud-pubsub<1.1.0,>=0.39.0\n",
            "  Downloading google_cloud_pubsub-1.0.2-py2.py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 56.0 MB/s \n",
            "\u001b[?25hCollecting google-cloud-videointelligence<1.14.0,>=1.8.0\n",
            "  Downloading google_cloud_videointelligence-1.13.0-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 54.9 MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigtable<1.1.0,>=0.31.1\n",
            "  Downloading google_cloud_bigtable-1.0.0-py2.py3-none-any.whl (232 kB)\n",
            "\u001b[K     |████████████████████████████████| 232 kB 54.7 MB/s \n",
            "\u001b[?25hCollecting google-cloud-vision<0.43.0,>=0.38.0\n",
            "  Downloading google_cloud_vision-0.42.0-py2.py3-none-any.whl (435 kB)\n",
            "\u001b[K     |████████████████████████████████| 435 kB 36.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.0.3)\n",
            "Collecting cachetools<4,>=3.1.0\n",
            "  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting google-cloud-dlp<=0.13.0,>=0.12.0\n",
            "  Downloading google_cloud_dlp-0.13.0-py2.py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<=1.24.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.21.0)\n",
            "Collecting google-cloud-datastore<1.8.0,>=1.7.1\n",
            "  Downloading google_cloud_datastore-1.7.4-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.10.1->tapas-table-parsing) (0.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.10.1->tapas-table-parsing) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle==1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.10.1->tapas-table-parsing) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.10.1->tapas-table-parsing) (4.4.2)\n",
            "Collecting fasteners>=0.14\n",
            "  Downloading fasteners-0.16.3-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<=1.24.0,>=1.6.0->apache-beam[gcp]==2.20.0->tapas-table-parsing) (0.4.1)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading grpc-google-iam-v1-0.12.3.tar.gz (13 kB)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.26.3)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.35.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.53.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (2.23.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (21.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (4.8)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.20.0->tapas-table-parsing) (0.6.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from kaggle<1.5.8->tapas-table-parsing) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle<1.5.8->tapas-table-parsing) (2021.10.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle<1.5.8->tapas-table-parsing) (4.62.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle<1.5.8->tapas-table-parsing) (5.0.2)\n",
            "Collecting pbr>=0.11\n",
            "  Downloading pbr-5.8.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 38.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk~=3.5->tapas-table-parsing) (1.1.0)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2021.11.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk~=3.5->tapas-table-parsing) (7.1.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (0.4.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (3.0.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (3.0.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn~=0.22.1->tapas-table-parsing) (1.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (0.37.0)\n",
            "Collecting numpy<2,>=1.14.3\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (0.12.0)\n",
            "Collecting gast>=0.3.2\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 42.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (1.13.3)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |████████████████████████████████| 454 kB 53.5 MB/s \n",
            "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 41.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (1.1.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (1.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (3.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (3.1.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (7.1.2)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 24.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (3.2.2)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 37.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (4.0.1)\n",
            "Collecting mlperf-compliance==0.0.10\n",
            "  Downloading mlperf_compliance-0.0.10-py3-none-any.whl (24 kB)\n",
            "Collecting typing==3.7.4.1\n",
            "  Downloading typing-3.7.4.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (0.12.0)\n",
            "Collecting tensorflow-model-optimization>=0.2.1\n",
            "  Downloading tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (0.29.24)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.6 MB 77 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (3.13)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (0.5.0)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (1.12.8)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (5.4.8)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.2.0->tapas-table-parsing) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.2.0->tapas-table-parsing) (3.0.1)\n",
            "Collecting google-api-python-client>=1.6.7\n",
            "  Downloading google_api_python_client-2.32.0-py2.py3-none-any.whl (7.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8 MB 36.1 MB/s \n",
            "\u001b[?25hCollecting google-auth-httplib2>=0.1.0\n",
            "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Collecting google-api-python-client>=1.6.7\n",
            "  Downloading google_api_python_client-2.31.0-py2.py3-none-any.whl (7.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8 MB 40.5 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.30.0-py2.py3-none-any.whl (7.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8 MB 38.6 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.29.0-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 19.7 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.28.0-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 1.3 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.27.0-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 21.5 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.26.1-py2.py3-none-any.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 11.4 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.26.0-py2.py3-none-any.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 13.0 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.25.0-py2.py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 8.4 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.24.0-py2.py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 20.6 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.23.0-py2.py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 24.8 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.22.0-py2.py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 41.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.21.0-py2.py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 38.6 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.20.0-py2.py3-none-any.whl (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 41.4 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.19.1-py2.py3-none-any.whl (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 40.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.19.0-py2.py3-none-any.whl (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 11.6 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.18.0-py2.py3-none-any.whl (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 694 kB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.17.0-py2.py3-none-any.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 17.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.15.0-py2.py3-none-any.whl (7.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2 MB 17.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.14.1-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 51.0 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.14.0-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 12.5 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.13.0-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 10.7 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.12.0-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 8.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.11.0-py2.py3-none-any.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 11.0 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.10.0-py2.py3-none-any.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 13.4 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.9.0-py2.py3-none-any.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 35.7 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.8.0-py2.py3-none-any.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 20.8 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.7.0-py2.py3-none-any.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 26.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.6.0-py2.py3-none-any.whl (7.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2 MB 29.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.5.0-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 21.3 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.4.0-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 8.0 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.3.0-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 11.6 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.2.0-py2.py3-none-any.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 43.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.1.0-py2.py3-none-any.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 10.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.0.2-py2.py3-none-any.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 15.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-1.12.7-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 22 kB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-1.12.6-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 32 kB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-1.12.5-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 8.6 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-1.12.4-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 9.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-1.12.3-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 8.8 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-1.12.2-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.2.1->tf-models-official~=2.2.0->tapas-table-parsing) (0.1.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official~=2.2.0->tapas-table-parsing) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official~=2.2.0->tapas-table-parsing) (0.11.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle<1.5.8->tapas-table-parsing) (1.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official~=2.2.0->tapas-table-parsing) (2.7.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official~=2.2.0->tapas-table-parsing) (5.4.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official~=2.2.0->tapas-table-parsing) (21.2.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official~=2.2.0->tapas-table-parsing) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official~=2.2.0->tapas-table-parsing) (1.4.0)\n",
            "Building wheels for collected packages: frozendict, avro-python3, dill, google-apitools, grpc-google-iam-v1, httplib2, kaggle, oauth2client, py-cpuinfo\n",
            "  Building wheel for frozendict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for frozendict: filename=frozendict-1.2-py3-none-any.whl size=3166 sha256=50b6349daa25f240fc951695228a5d0b543ea1e73c25adc6437b4319a9777020\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/17/69/ac196dd181e620bba5fae5488e4fd6366a7316dce13cf88776\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=d1613d5aef0d8ad840ed4ab8611f661e6bfcd69e2e63582c122c278cbc05ece1\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=e9a8d350b9f383a56ce0fa0a18833799ade7fbc4bba5f291edd160e598aebaa5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.28-py3-none-any.whl size=130108 sha256=6e323cdaff694923268f2c9c6c7416d76d82e1c23971b4f6b232bc6a4b52a0d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/3b/69/ecd8e6ae89d9d71102a58962c29faa7a9467ba45f99f205920\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-py3-none-any.whl size=18515 sha256=498ed44587bf9bc9f1314b9487b875a8312b5636d16c5e99950ca68adbc372f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/ee/67/2e444183030cb8d31ce8b34cee34a7afdbd3ba5959ea846380\n",
            "  Building wheel for httplib2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for httplib2: filename=httplib2-0.12.0-py3-none-any.whl size=93464 sha256=215174bc51962a025e9517feb6d87e6f0516f9f6129795ab1fd5f244601a12b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/e7/b6/0dd30343ceca921cfbd91f355041bd9c69e0f40b49f25b7b8a\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.6-py3-none-any.whl size=72857 sha256=7c33089722e1c4d1a4c98b7b2df6af7f68f70e0e896e20c0352d45c82e75e3bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/e7/e7/eb3c3d514c33294d77ddd5a856bdd58dc9c1fabbed59a02a2b\n",
            "  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oauth2client: filename=oauth2client-3.0.0-py3-none-any.whl size=106375 sha256=07d33d6b9eee46844198e06e46c394f0be6f3a8e61c254de5427a3785f4b2afc\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/73/7a/3b3f76a2142176605ff38fbca574327962c71e25a43197a4c1\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22258 sha256=9c8af92161f5cc243439025372e52720a5fed20757e8fd0589e017d9992fa6c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "Successfully built frozendict avro-python3 dill google-apitools grpc-google-iam-v1 httplib2 kaggle oauth2client py-cpuinfo\n",
            "Installing collected packages: typing-extensions, cachetools, pbr, numpy, httplib2, grpcio-gcp, tensorflow-estimator, tensorboard, pyarrow, oauth2client, mock, hdfs, h5py, grpc-google-iam-v1, gast, fasteners, fastavro, dill, avro-python3, typing, tensorflow-model-optimization, tensorflow-addons, tensorflow, sentencepiece, regex, py-cpuinfo, pandas, opencv-python-headless, mlperf-compliance, kaggle, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-apitools, google-api-python-client, dataclasses, apache-beam, tf-slim, tf-models-official, tensorflow-probability, scikit-learn, nltk, frozendict, tapas-table-parsing\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 4.2.4\n",
            "    Uninstalling cachetools-4.2.4:\n",
            "      Successfully uninstalled cachetools-4.2.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: httplib2\n",
            "    Found existing installation: httplib2 0.17.4\n",
            "    Uninstalling httplib2-0.17.4:\n",
            "      Successfully uninstalled httplib2-0.17.4\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "  Attempting uninstall: oauth2client\n",
            "    Found existing installation: oauth2client 4.1.3\n",
            "    Uninstalling oauth2client-4.1.3:\n",
            "      Successfully uninstalled oauth2client-4.1.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "  Attempting uninstall: google-cloud-language\n",
            "    Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "  Attempting uninstall: google-cloud-datastore\n",
            "    Found existing installation: google-cloud-datastore 1.8.0\n",
            "    Uninstalling google-cloud-datastore-1.8.0:\n",
            "      Successfully uninstalled google-cloud-datastore-1.8.0\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.15.0\n",
            "    Uninstalling tensorflow-probability-0.15.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.15.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc3 3.11.4 requires cachetools>=4.2.1, but you have cachetools 3.1.1 which is incompatible.\n",
            "pydrive 1.3.1 requires oauth2client>=4.0.0, but you have oauth2client 3.0.0 which is incompatible.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.0.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed apache-beam-2.20.0 avro-python3-1.9.2.1 cachetools-3.1.1 dataclasses-0.6 dill-0.3.1.1 fastavro-0.21.24 fasteners-0.16.3 frozendict-1.2 gast-0.3.3 google-api-python-client-1.12.2 google-apitools-0.5.28 google-cloud-bigtable-1.0.0 google-cloud-datastore-1.7.4 google-cloud-dlp-0.13.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.0.2 google-cloud-spanner-1.13.0 google-cloud-videointelligence-1.13.0 google-cloud-vision-0.42.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 h5py-2.10.0 hdfs-2.6.0 httplib2-0.12.0 kaggle-1.5.6 mlperf-compliance-0.0.10 mock-2.0.0 nltk-3.6.5 numpy-1.18.5 oauth2client-3.0.0 opencv-python-headless-4.5.4.60 pandas-1.0.5 pbr-5.8.0 py-cpuinfo-8.0.0 pyarrow-0.16.0 regex-2021.11.10 scikit-learn-0.22.2.post1 sentencepiece-0.1.96 tapas-table-parsing-0.0.1.dev0 tensorboard-2.2.2 tensorflow-2.2.3 tensorflow-addons-0.15.0 tensorflow-estimator-2.2.0 tensorflow-model-optimization-0.7.0 tensorflow-probability-0.10.1 tf-models-official-2.2.2 tf-slim-1.1.0 typing-3.7.4.1 typing-extensions-3.7.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cachetools",
                  "google",
                  "numpy",
                  "pandas",
                  "pyarrow",
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7We9ofHuFMuk"
      },
      "source": [
        "# Fetch models fom Google Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA1jUByqyUNB"
      },
      "source": [
        "Next we can get pretrained checkpoint from Google Storage. For the sake of speed, this is base sized model trained on [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253). Note that best results in the paper were obtained with a large model, with 24 layers instead of 12."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B10C0Yz6gQyD",
        "outputId": "2039af14-2a3e-4e62-83ae-e63b86dec400"
      },
      "source": [
        "\n",
        "! gsutil cp gs://tapas_models/2020_04_21/tapas_sqa_base.zip . && unzip tapas_sqa_base.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://tapas_models/2020_04_21/tapas_sqa_base.zip...\n",
            "/ [1 files][  1.0 GiB/  1.0 GiB]   44.4 MiB/s                                   \n",
            "Operation completed over 1 objects/1.0 GiB.                                      \n",
            "Archive:  tapas_sqa_base.zip\n",
            "   creating: tapas_sqa_base/\n",
            "  inflating: tapas_sqa_base/model.ckpt.data-00000-of-00001  \n",
            "  inflating: tapas_sqa_base/model.ckpt.index  \n",
            "  inflating: tapas_sqa_base/README.txt  \n",
            "  inflating: tapas_sqa_base/vocab.txt  \n",
            "  inflating: tapas_sqa_base/bert_config.json  \n",
            "  inflating: tapas_sqa_base/model.ckpt.meta  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3107bGlGm7d"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnUjDlLqDd3m"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import os \n",
        "import shutil\n",
        "import csv\n",
        "import pandas as pd\n",
        "import IPython\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aml6oLFl1dSt"
      },
      "source": [
        "from tapas.utils import tf_example_utils\n",
        "from tapas.protos import interaction_pb2\n",
        "from tapas.utils import number_annotation_utils\n",
        "from tapas.scripts import prediction_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbMUYT1bKMp9"
      },
      "source": [
        "# Load checkpoint for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHJqO-pyxwTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55645d23-3011-4424-bd88-4cd7297fe9b6"
      },
      "source": [
        "import psycopg2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJKuRn_wxtns"
      },
      "source": [
        "# access the postgresql server\n",
        "conn = psycopg2.connect(\n",
        "    host=\"codd04.research.northwestern.edu\",\n",
        "    port = \"5433\",\n",
        "    database=\"postgres\",\n",
        "    user=\"cpdbstudent\",\n",
        "    password=\"DataSci4AI\")\n",
        "cursor = conn.cursor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk4-kzU7x9s7"
      },
      "source": [
        "sql = '''\n",
        "DROP TABLE IF EXISTS da_category_ids;\n",
        "CREATE TEMP TABLE da_category_ids AS (\n",
        "(SELECT data_officerallegation.id,data_allegationcategory.allegation_name,data_allegationcategory.category\n",
        "FROM data_officerallegation\n",
        "join data_allegationcategory on data_officerallegation.allegation_category_id = data_allegationcategory.id\n",
        "WHERE data_allegationcategory.category = 'Drug / Alcohol Abuse' OR data_allegationcategory.category = 'Medical' or allegation_name LIKE 'Medical Roll%'\n",
        "OR data_allegationcategory.category_code IN ('024', '003', '003A', '003B', '003C', '003D', '003E')));\n",
        "\n",
        "SELECT gender,race,birth_year,first_name || ' ' || last_name officer_name,allegation_count,sustained_count,current_salary\n",
        "FROM data_officer\n",
        "join da_category_ids on data_officer.id=da_category_ids.id\n",
        "where data_officer.current_salary IS NOT NULL and allegation_count>2 and sustained_count>1 order by officer_name desc;\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL5FkrmJyI3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce172125-b24d-458d-b557-7317f524b358"
      },
      "source": [
        "cursor.execute(sql)\n",
        "colnames = ['gender', 'race', 'birth year', 'officer name', 'allegation count', 'sustained count', 'current salary']\n",
        "edges = cursor.fetchall()\n",
        "res = [[str(x) for x in list(ele)] for ele in edges]\n",
        "res.insert(0, colnames)\n",
        "print(res)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['gender', 'race', 'birth year', 'officer name', 'allegation count', 'sustained count', 'current salary'], ['M', 'Hispanic', '1952', 'Wilman Dones', '23', '6', '82008'], ['M', 'Black', '1963', 'Vernard Ross', '23', '3', '107988'], ['M', 'White', '1957', 'Thomas Motzny', '31', '2', '102978'], ['M', 'White', '1949', 'Thomas Biggane', '30', '2', '82878'], ['M', 'White', '1969', 'Steven Nowicki', '27', '3', '111474'], ['M', 'White', '1970', 'Steven Bechina', '43', '4', '107988'], ['M', 'White', '1948', 'Ronald Blake', '6', '2', '83604'], ['M', 'Black', '1948', 'Rollins Johnson', '6', '2', '88260'], ['M', 'Hispanic', '1985', 'Roger Farias', '14', '2', '84054'], ['M', 'Black', '1964', 'Ricky Bean', '11', '3', '100980'], ['M', 'White', '1961', 'Raymond Gadomski', '17', '3', '96060'], ['M', 'Black', '1953', 'Prentiss Jackson', '15', '5', '102978'], ['M', 'White', '1962', 'Philip Paluch', '56', '2', '111474'], ['F', 'Black', '1950', 'Paularie Draine', '7', '2', '78012'], ['F', 'Black', '1948', 'Patricia Ballentine', '16', '6', '79926'], ['M', 'Hispanic', '1971', 'Orlando Fonseca', '7', '2', '83616'], ['M', 'White', '1955', 'Nicholas Dimaggio', '23', '15', '87888'], ['M', 'Black', '1951', 'Michael Overstreet', '56', '10', '78006'], ['M', 'Black', '1973', 'Marvin Randolph', '14', '3', '90024'], ['M', 'White', '1947', 'Marshall Pufundt', '23', '2', '58884'], ['M', 'Black', '1959', 'Marco Johnson', '24', '6', '86130'], ['M', 'Hispanic', '1967', 'Luis Lopez', '8', '3', '93354'], ['F', 'Black', '1961', 'Lisa William-Handley', '13', '2', '90618'], ['F', 'Black', '1963', 'Linda Brumfield', '17', '8', '68262'], ['M', 'Black', '1956', 'Larry Dotson', '43', '3', '90540'], ['F', 'Black', '1970', 'Lakisa Anderson', '23', '4', '93354'], ['F', 'Black', '1964', 'Kimberly Hill', '24', '3', '86130'], ['M', 'White', '1960', 'Kevin Keyes', '32', '11', '69270'], ['M', 'White', '1979', 'Joseph Thompson', '9', '3', '101442'], ['M', 'White', '1961', 'Joseph Battaglia', '66', '4', '83706'], ['M', 'Black', '1957', 'John Brownridge', '54', '4', '100980'], ['M', 'Hispanic', '1959', 'Jesus Avila', '22', '4', '95106'], ['F', 'Black', '1965', 'Jeanetta Brown Cunningha', '5', '4', '99888'], ['F', 'White', '1981', 'Jacklyn Mueller', '27', '2', '87006'], ['M', 'Hispanic', '1965', 'Irwin Negron', '33', '5', '96060'], ['M', 'Black', '1968', 'Haki Akintunde', '12', '4', '70656'], ['M', 'Black', '1968', 'Frederick Anthony', '29', '2', '92430'], ['M', 'White', '1963', 'Emil Bux', '13', '3', '93354'], ['M', 'Black', '1959', 'Donald Banks', '10', '2', '92316'], ['M', 'White', '1954', 'Dennis Peca', '11', '2', '90456'], ['M', 'Black', '1970', 'Brian Burton', '42', '3', '93354'], ['M', 'Black', '1962', 'Anthony Singleton', '5', '2', '90024'], ['F', 'Black', '1943', 'Anita Ashton', '23', '4', '71682'], ['M', 'Black', '1949', 'Alvin Ward', '11', '5', '69264'], ['M', 'Black', '1963', 'Alvin Greenup', '27', '3', '93354']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKfxspnVFPsc"
      },
      "source": [
        "os.makedirs('results/sqa/tf_examples', exist_ok=True)\n",
        "os.makedirs('results/sqa/model', exist_ok=True)\n",
        "with open('results/sqa/model/checkpoint', 'w') as f:\n",
        "  f.write('model_checkpoint_path: \"model.ckpt-0\"')\n",
        "for suffix in ['.data-00000-of-00001', '.index', '.meta']:\n",
        "  shutil.copyfile(f'tapas_sqa_base/model.ckpt{suffix}', f'results/sqa/model/model.ckpt-0{suffix}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RlvgDAmCNtP"
      },
      "source": [
        "max_seq_length = 512\n",
        "vocab_file = \"tapas_sqa_base/vocab.txt\"\n",
        "config = tf_example_utils.ClassifierConversionConfig(\n",
        "    vocab_file=vocab_file,\n",
        "    max_seq_length=max_seq_length,\n",
        "    max_column_id=max_seq_length,\n",
        "    max_row_id=max_seq_length,\n",
        "    strip_column_names=False,\n",
        "    add_aggregation_candidates=False,\n",
        ")\n",
        "converter = tf_example_utils.ToClassifierTensorflowExample(config)\n",
        "\n",
        "def convert_interactions_to_examples(tables_and_queries):\n",
        "  \"\"\"Calls Tapas converter to convert interaction to example.\"\"\"\n",
        "  for idx, (table, queries) in enumerate(tables_and_queries):\n",
        "    interaction = interaction_pb2.Interaction()\n",
        "    for position, query in enumerate(queries):\n",
        "      question = interaction.questions.add()\n",
        "      question.original_text = query\n",
        "      question.id = f\"{idx}-0_{position}\"\n",
        "    for header in table[0]:\n",
        "      interaction.table.columns.add().text = header\n",
        "    for line in table[1:]:\n",
        "      row = interaction.table.rows.add()\n",
        "      for cell in line:\n",
        "        row.cells.add().text = cell\n",
        "    number_annotation_utils.add_numeric_values(interaction)\n",
        "    for i in range(len(interaction.questions)):\n",
        "      try:\n",
        "        yield converter.convert(interaction, i)\n",
        "      except ValueError as e:\n",
        "        print(f\"Can't convert interaction: {interaction.id} error: {e}\")\n",
        "        \n",
        "def write_tf_example(filename, examples):\n",
        "  with tf.io.TFRecordWriter(filename) as writer:\n",
        "    for example in examples:\n",
        "      writer.write(example.SerializeToString())\n",
        "\n",
        "def predict(table_data, queries):\n",
        "  answers=[]\n",
        "  table = res\n",
        "  examples = convert_interactions_to_examples([(table, queries)])\n",
        "  write_tf_example(\"results/sqa/tf_examples/test.tfrecord\", examples)\n",
        "  write_tf_example(\"results/sqa/tf_examples/random-split-1-dev.tfrecord\", [])\n",
        "  \n",
        "  ! python -m tapas.run_task_main \\\n",
        "    --task=\"SQA\" \\\n",
        "    --output_dir=\"results\" \\\n",
        "    --noloop_predict \\\n",
        "    --test_batch_size={len(queries)} \\\n",
        "    --tapas_verbosity=\"ERROR\" \\\n",
        "    --compression_type= \\\n",
        "    --init_checkpoint=\"tapas_sqa_base/model.ckpt\" \\\n",
        "    --bert_config_file=\"tapas_sqa_base/bert_config.json\" \\\n",
        "    --mode=\"predict\" 2> error\n",
        "\n",
        "\n",
        "  results_path = \"results/sqa/model/test_sequence.tsv\"\n",
        "  all_coordinates = []\n",
        "  df = pd.DataFrame(table[1:], columns=table[0])\n",
        "  with open(results_path) as csvfile:\n",
        "    reader = csv.DictReader(csvfile, delimiter='\\t')\n",
        "    for row in reader:\n",
        "      coordinates = prediction_utils.parse_coordinates(row[\"answer_coordinates\"])\n",
        "      all_coordinates.append(coordinates)\n",
        "      answers.append(', '.join([table[row + 1][col] for row, col in coordinates]))\n",
        "      position = int(row['position'])\n",
        "  return answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfgxaZlOFE5n"
      },
      "source": [
        "Get sql column data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPHSrkfaBxJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ca387ee-c2bf-4c23-a1e4-89f769a890bd"
      },
      "source": [
        "officer_name=[el[3] for el in res][1:]\n",
        "print(officer_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Wilman Dones', 'Vernard Ross', 'Thomas Motzny', 'Thomas Biggane', 'Steven Nowicki', 'Steven Bechina', 'Ronald Blake', 'Rollins Johnson', 'Roger Farias', 'Ricky Bean', 'Raymond Gadomski', 'Prentiss Jackson', 'Philip Paluch', 'Paularie Draine', 'Patricia Ballentine', 'Orlando Fonseca', 'Nicholas Dimaggio', 'Michael Overstreet', 'Marvin Randolph', 'Marshall Pufundt', 'Marco Johnson', 'Luis Lopez', 'Lisa William-Handley', 'Linda Brumfield', 'Larry Dotson', 'Lakisa Anderson', 'Kimberly Hill', 'Kevin Keyes', 'Joseph Thompson', 'Joseph Battaglia', 'John Brownridge', 'Jesus Avila', 'Jeanetta Brown Cunningha', 'Jacklyn Mueller', 'Irwin Negron', 'Haki Akintunde', 'Frederick Anthony', 'Emil Bux', 'Donald Banks', 'Dennis Peca', 'Brian Burton', 'Anthony Singleton', 'Anita Ashton', 'Alvin Ward', 'Alvin Greenup']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goFL-yXvCOV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcfbc1d8-9f96-4a2b-ea21-8723320327ee"
      },
      "source": [
        "race=[]\n",
        "for x in officer_name:\n",
        "  race.append(\"What is the race of \"+x+\"?\")\n",
        "print(race)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What is the race of Wilman Dones?', 'What is the race of Vernard Ross?', 'What is the race of Thomas Motzny?', 'What is the race of Thomas Biggane?', 'What is the race of Steven Nowicki?', 'What is the race of Steven Bechina?', 'What is the race of Ronald Blake?', 'What is the race of Rollins Johnson?', 'What is the race of Roger Farias?', 'What is the race of Ricky Bean?', 'What is the race of Raymond Gadomski?', 'What is the race of Prentiss Jackson?', 'What is the race of Philip Paluch?', 'What is the race of Paularie Draine?', 'What is the race of Patricia Ballentine?', 'What is the race of Orlando Fonseca?', 'What is the race of Nicholas Dimaggio?', 'What is the race of Michael Overstreet?', 'What is the race of Marvin Randolph?', 'What is the race of Marshall Pufundt?', 'What is the race of Marco Johnson?', 'What is the race of Luis Lopez?', 'What is the race of Lisa William-Handley?', 'What is the race of Linda Brumfield?', 'What is the race of Larry Dotson?', 'What is the race of Lakisa Anderson?', 'What is the race of Kimberly Hill?', 'What is the race of Kevin Keyes?', 'What is the race of Joseph Thompson?', 'What is the race of Joseph Battaglia?', 'What is the race of John Brownridge?', 'What is the race of Jesus Avila?', 'What is the race of Jeanetta Brown Cunningha?', 'What is the race of Jacklyn Mueller?', 'What is the race of Irwin Negron?', 'What is the race of Haki Akintunde?', 'What is the race of Frederick Anthony?', 'What is the race of Emil Bux?', 'What is the race of Donald Banks?', 'What is the race of Dennis Peca?', 'What is the race of Brian Burton?', 'What is the race of Anthony Singleton?', 'What is the race of Anita Ashton?', 'What is the race of Alvin Ward?', 'What is the race of Alvin Greenup?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2_wvrTXClXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "705419f6-f2dc-4725-ede4-0543ce323045"
      },
      "source": [
        "gender=[]\n",
        "for x in officer_name:\n",
        "  gender.append(\"What is the gender of \"+x+\"?\")\n",
        "print(gender)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What is the gender of Wilman Dones?', 'What is the gender of Vernard Ross?', 'What is the gender of Thomas Motzny?', 'What is the gender of Thomas Biggane?', 'What is the gender of Steven Nowicki?', 'What is the gender of Steven Bechina?', 'What is the gender of Ronald Blake?', 'What is the gender of Rollins Johnson?', 'What is the gender of Roger Farias?', 'What is the gender of Ricky Bean?', 'What is the gender of Raymond Gadomski?', 'What is the gender of Prentiss Jackson?', 'What is the gender of Philip Paluch?', 'What is the gender of Paularie Draine?', 'What is the gender of Patricia Ballentine?', 'What is the gender of Orlando Fonseca?', 'What is the gender of Nicholas Dimaggio?', 'What is the gender of Michael Overstreet?', 'What is the gender of Marvin Randolph?', 'What is the gender of Marshall Pufundt?', 'What is the gender of Marco Johnson?', 'What is the gender of Luis Lopez?', 'What is the gender of Lisa William-Handley?', 'What is the gender of Linda Brumfield?', 'What is the gender of Larry Dotson?', 'What is the gender of Lakisa Anderson?', 'What is the gender of Kimberly Hill?', 'What is the gender of Kevin Keyes?', 'What is the gender of Joseph Thompson?', 'What is the gender of Joseph Battaglia?', 'What is the gender of John Brownridge?', 'What is the gender of Jesus Avila?', 'What is the gender of Jeanetta Brown Cunningha?', 'What is the gender of Jacklyn Mueller?', 'What is the gender of Irwin Negron?', 'What is the gender of Haki Akintunde?', 'What is the gender of Frederick Anthony?', 'What is the gender of Emil Bux?', 'What is the gender of Donald Banks?', 'What is the gender of Dennis Peca?', 'What is the gender of Brian Burton?', 'What is the gender of Anthony Singleton?', 'What is the gender of Anita Ashton?', 'What is the gender of Alvin Ward?', 'What is the gender of Alvin Greenup?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66ofjLvkCk9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c6ec33-6cb0-44fe-f9fe-ecf0425fde42"
      },
      "source": [
        "birth_year=[]\n",
        "for x in officer_name:\n",
        "  birth_year.append(\"What is the birth year of \"+x+\"?\")\n",
        "print(birth_year)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What is the birth year of Wilman Dones?', 'What is the birth year of Vernard Ross?', 'What is the birth year of Thomas Motzny?', 'What is the birth year of Thomas Biggane?', 'What is the birth year of Steven Nowicki?', 'What is the birth year of Steven Bechina?', 'What is the birth year of Ronald Blake?', 'What is the birth year of Rollins Johnson?', 'What is the birth year of Roger Farias?', 'What is the birth year of Ricky Bean?', 'What is the birth year of Raymond Gadomski?', 'What is the birth year of Prentiss Jackson?', 'What is the birth year of Philip Paluch?', 'What is the birth year of Paularie Draine?', 'What is the birth year of Patricia Ballentine?', 'What is the birth year of Orlando Fonseca?', 'What is the birth year of Nicholas Dimaggio?', 'What is the birth year of Michael Overstreet?', 'What is the birth year of Marvin Randolph?', 'What is the birth year of Marshall Pufundt?', 'What is the birth year of Marco Johnson?', 'What is the birth year of Luis Lopez?', 'What is the birth year of Lisa William-Handley?', 'What is the birth year of Linda Brumfield?', 'What is the birth year of Larry Dotson?', 'What is the birth year of Lakisa Anderson?', 'What is the birth year of Kimberly Hill?', 'What is the birth year of Kevin Keyes?', 'What is the birth year of Joseph Thompson?', 'What is the birth year of Joseph Battaglia?', 'What is the birth year of John Brownridge?', 'What is the birth year of Jesus Avila?', 'What is the birth year of Jeanetta Brown Cunningha?', 'What is the birth year of Jacklyn Mueller?', 'What is the birth year of Irwin Negron?', 'What is the birth year of Haki Akintunde?', 'What is the birth year of Frederick Anthony?', 'What is the birth year of Emil Bux?', 'What is the birth year of Donald Banks?', 'What is the birth year of Dennis Peca?', 'What is the birth year of Brian Burton?', 'What is the birth year of Anthony Singleton?', 'What is the birth year of Anita Ashton?', 'What is the birth year of Alvin Ward?', 'What is the birth year of Alvin Greenup?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4TCE6j5Ck3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2802970c-db71-4729-98e7-714c87363455"
      },
      "source": [
        "allegation_count=[]\n",
        "for x in officer_name:\n",
        "  allegation_count.append(\"What is the allegation count of \"+x+\"?\")\n",
        "print(allegation_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What is the allegation count of Wilman Dones?', 'What is the allegation count of Vernard Ross?', 'What is the allegation count of Thomas Motzny?', 'What is the allegation count of Thomas Biggane?', 'What is the allegation count of Steven Nowicki?', 'What is the allegation count of Steven Bechina?', 'What is the allegation count of Ronald Blake?', 'What is the allegation count of Rollins Johnson?', 'What is the allegation count of Roger Farias?', 'What is the allegation count of Ricky Bean?', 'What is the allegation count of Raymond Gadomski?', 'What is the allegation count of Prentiss Jackson?', 'What is the allegation count of Philip Paluch?', 'What is the allegation count of Paularie Draine?', 'What is the allegation count of Patricia Ballentine?', 'What is the allegation count of Orlando Fonseca?', 'What is the allegation count of Nicholas Dimaggio?', 'What is the allegation count of Michael Overstreet?', 'What is the allegation count of Marvin Randolph?', 'What is the allegation count of Marshall Pufundt?', 'What is the allegation count of Marco Johnson?', 'What is the allegation count of Luis Lopez?', 'What is the allegation count of Lisa William-Handley?', 'What is the allegation count of Linda Brumfield?', 'What is the allegation count of Larry Dotson?', 'What is the allegation count of Lakisa Anderson?', 'What is the allegation count of Kimberly Hill?', 'What is the allegation count of Kevin Keyes?', 'What is the allegation count of Joseph Thompson?', 'What is the allegation count of Joseph Battaglia?', 'What is the allegation count of John Brownridge?', 'What is the allegation count of Jesus Avila?', 'What is the allegation count of Jeanetta Brown Cunningha?', 'What is the allegation count of Jacklyn Mueller?', 'What is the allegation count of Irwin Negron?', 'What is the allegation count of Haki Akintunde?', 'What is the allegation count of Frederick Anthony?', 'What is the allegation count of Emil Bux?', 'What is the allegation count of Donald Banks?', 'What is the allegation count of Dennis Peca?', 'What is the allegation count of Brian Burton?', 'What is the allegation count of Anthony Singleton?', 'What is the allegation count of Anita Ashton?', 'What is the allegation count of Alvin Ward?', 'What is the allegation count of Alvin Greenup?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqkTXb74Cko0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7cceb03-199e-4ab1-98dd-4e940901169c"
      },
      "source": [
        "sustained_count=[]\n",
        "for x in officer_name:\n",
        "  sustained_count.append(\"What is the sustained count of \"+x+\"?\")\n",
        "print(sustained_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What is the sustained count of Wilman Dones?', 'What is the sustained count of Vernard Ross?', 'What is the sustained count of Thomas Motzny?', 'What is the sustained count of Thomas Biggane?', 'What is the sustained count of Steven Nowicki?', 'What is the sustained count of Steven Bechina?', 'What is the sustained count of Ronald Blake?', 'What is the sustained count of Rollins Johnson?', 'What is the sustained count of Roger Farias?', 'What is the sustained count of Ricky Bean?', 'What is the sustained count of Raymond Gadomski?', 'What is the sustained count of Prentiss Jackson?', 'What is the sustained count of Philip Paluch?', 'What is the sustained count of Paularie Draine?', 'What is the sustained count of Patricia Ballentine?', 'What is the sustained count of Orlando Fonseca?', 'What is the sustained count of Nicholas Dimaggio?', 'What is the sustained count of Michael Overstreet?', 'What is the sustained count of Marvin Randolph?', 'What is the sustained count of Marshall Pufundt?', 'What is the sustained count of Marco Johnson?', 'What is the sustained count of Luis Lopez?', 'What is the sustained count of Lisa William-Handley?', 'What is the sustained count of Linda Brumfield?', 'What is the sustained count of Larry Dotson?', 'What is the sustained count of Lakisa Anderson?', 'What is the sustained count of Kimberly Hill?', 'What is the sustained count of Kevin Keyes?', 'What is the sustained count of Joseph Thompson?', 'What is the sustained count of Joseph Battaglia?', 'What is the sustained count of John Brownridge?', 'What is the sustained count of Jesus Avila?', 'What is the sustained count of Jeanetta Brown Cunningha?', 'What is the sustained count of Jacklyn Mueller?', 'What is the sustained count of Irwin Negron?', 'What is the sustained count of Haki Akintunde?', 'What is the sustained count of Frederick Anthony?', 'What is the sustained count of Emil Bux?', 'What is the sustained count of Donald Banks?', 'What is the sustained count of Dennis Peca?', 'What is the sustained count of Brian Burton?', 'What is the sustained count of Anthony Singleton?', 'What is the sustained count of Anita Ashton?', 'What is the sustained count of Alvin Ward?', 'What is the sustained count of Alvin Greenup?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peREIEwMDBwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64657e40-5547-46a5-e60e-efced99f50b8"
      },
      "source": [
        "current_salary=[]\n",
        "for x in officer_name:\n",
        "  current_salary.append(\"What is the current salary of \"+x+\"?\")\n",
        "print(current_salary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What is the current salary of Wilman Dones?', 'What is the current salary of Vernard Ross?', 'What is the current salary of Thomas Motzny?', 'What is the current salary of Thomas Biggane?', 'What is the current salary of Steven Nowicki?', 'What is the current salary of Steven Bechina?', 'What is the current salary of Ronald Blake?', 'What is the current salary of Rollins Johnson?', 'What is the current salary of Roger Farias?', 'What is the current salary of Ricky Bean?', 'What is the current salary of Raymond Gadomski?', 'What is the current salary of Prentiss Jackson?', 'What is the current salary of Philip Paluch?', 'What is the current salary of Paularie Draine?', 'What is the current salary of Patricia Ballentine?', 'What is the current salary of Orlando Fonseca?', 'What is the current salary of Nicholas Dimaggio?', 'What is the current salary of Michael Overstreet?', 'What is the current salary of Marvin Randolph?', 'What is the current salary of Marshall Pufundt?', 'What is the current salary of Marco Johnson?', 'What is the current salary of Luis Lopez?', 'What is the current salary of Lisa William-Handley?', 'What is the current salary of Linda Brumfield?', 'What is the current salary of Larry Dotson?', 'What is the current salary of Lakisa Anderson?', 'What is the current salary of Kimberly Hill?', 'What is the current salary of Kevin Keyes?', 'What is the current salary of Joseph Thompson?', 'What is the current salary of Joseph Battaglia?', 'What is the current salary of John Brownridge?', 'What is the current salary of Jesus Avila?', 'What is the current salary of Jeanetta Brown Cunningha?', 'What is the current salary of Jacklyn Mueller?', 'What is the current salary of Irwin Negron?', 'What is the current salary of Haki Akintunde?', 'What is the current salary of Frederick Anthony?', 'What is the current salary of Emil Bux?', 'What is the current salary of Donald Banks?', 'What is the current salary of Dennis Peca?', 'What is the current salary of Brian Burton?', 'What is the current salary of Anthony Singleton?', 'What is the current salary of Anita Ashton?', 'What is the current salary of Alvin Ward?', 'What is the current salary of Alvin Greenup?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P8wY-BQqacJ"
      },
      "source": [
        "# **Choose your column name in select_col**\n",
        "```\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i8lraNonSsh",
        "outputId": "294449b2-d26d-4240-cb29-096ec10eea13"
      },
      "source": [
        "select_col = 'birth_year'\n",
        "predict_col_list=[gender,race,birth_year,allegation_count,sustained_count,current_salary]\n",
        "predict_col_str=['gender','race','birth_year','allegation_count','sustained_count','current_salary']\n",
        "\n",
        "for x in range(0,len(predict_col_list)):\n",
        "  if predict_col_str[x]==select_col:\n",
        "    col_num=x\n",
        "    col_name=predict_col_list[x]\n",
        "    print(col_name[0])\n",
        "print(col_num,col_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the birth year of Wilman Dones?\n",
            "2 ['What is the birth year of Wilman Dones?', 'What is the birth year of Vernard Ross?', 'What is the birth year of Thomas Motzny?', 'What is the birth year of Thomas Biggane?', 'What is the birth year of Steven Nowicki?', 'What is the birth year of Steven Bechina?', 'What is the birth year of Ronald Blake?', 'What is the birth year of Rollins Johnson?', 'What is the birth year of Roger Farias?', 'What is the birth year of Ricky Bean?', 'What is the birth year of Raymond Gadomski?', 'What is the birth year of Prentiss Jackson?', 'What is the birth year of Philip Paluch?', 'What is the birth year of Paularie Draine?', 'What is the birth year of Patricia Ballentine?', 'What is the birth year of Orlando Fonseca?', 'What is the birth year of Nicholas Dimaggio?', 'What is the birth year of Michael Overstreet?', 'What is the birth year of Marvin Randolph?', 'What is the birth year of Marshall Pufundt?', 'What is the birth year of Marco Johnson?', 'What is the birth year of Luis Lopez?', 'What is the birth year of Lisa William-Handley?', 'What is the birth year of Linda Brumfield?', 'What is the birth year of Larry Dotson?', 'What is the birth year of Lakisa Anderson?', 'What is the birth year of Kimberly Hill?', 'What is the birth year of Kevin Keyes?', 'What is the birth year of Joseph Thompson?', 'What is the birth year of Joseph Battaglia?', 'What is the birth year of John Brownridge?', 'What is the birth year of Jesus Avila?', 'What is the birth year of Jeanetta Brown Cunningha?', 'What is the birth year of Jacklyn Mueller?', 'What is the birth year of Irwin Negron?', 'What is the birth year of Haki Akintunde?', 'What is the birth year of Frederick Anthony?', 'What is the birth year of Emil Bux?', 'What is the birth year of Donald Banks?', 'What is the birth year of Dennis Peca?', 'What is the birth year of Brian Burton?', 'What is the birth year of Anthony Singleton?', 'What is the birth year of Anita Ashton?', 'What is the birth year of Alvin Ward?', 'What is the birth year of Alvin Greenup?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnfXbs0LEjx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "681e4b28-d147-4e90-f48a-1bfeecf24c6a"
      },
      "source": [
        "column_number=col_num\n",
        "data_query=[el[column_number] for el in res][1:]\n",
        "print(data_query)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1952', '1963', '1957', '1949', '1969', '1970', '1948', '1948', '1985', '1964', '1961', '1953', '1962', '1950', '1948', '1971', '1955', '1951', '1973', '1947', '1959', '1967', '1961', '1963', '1956', '1970', '1964', '1960', '1979', '1961', '1957', '1959', '1965', '1981', '1965', '1968', '1968', '1963', '1959', '1954', '1970', '1962', '1943', '1949', '1963']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVoHtXCykmJn",
        "outputId": "73ea907c-ad06-48ee-aeb8-fcca0a42e366"
      },
      "source": [
        "print(col_name[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the birth year of Wilman Dones?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqu-I-M9QaoA"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25Gu57hjE-BN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dcc7ff0-5ee4-46d3-b458-8d2e8488aa1d"
      },
      "source": [
        "result = predict('', col_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is_built_with_cuda: True\n",
            "is_gpu_available: True\n",
            "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Training or predicting ...\n",
            "Evaluation finished after training step 0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lg8qL8nmLsB",
        "outputId": "9b07eb33-323d-4ede-b01b-ccd57a9e0f60"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1979, 1952',\n",
              " '1963',\n",
              " '1957',\n",
              " '1957',\n",
              " '1969, 1970',\n",
              " '1969',\n",
              " '1968',\n",
              " '1948, 1943',\n",
              " '1985',\n",
              " '1964',\n",
              " '1956, 1961',\n",
              " '1953',\n",
              " '1962',\n",
              " '1950',\n",
              " '1948',\n",
              " '1971',\n",
              " '1955',\n",
              " '1951',\n",
              " '1973',\n",
              " '1947',\n",
              " '1959',\n",
              " '1967',\n",
              " '1961',\n",
              " '1963',\n",
              " '1956, 1961',\n",
              " '1970',\n",
              " '1948, 1964',\n",
              " '1960, 1973',\n",
              " '1973',\n",
              " '1961',\n",
              " '1985',\n",
              " '1985',\n",
              " '1949, 1948',\n",
              " '1968',\n",
              " '1968',\n",
              " '1968, 1948',\n",
              " '1968',\n",
              " '1968, 1985',\n",
              " '1985',\n",
              " '1985',\n",
              " '1985',\n",
              " '1949',\n",
              " '',\n",
              " '',\n",
              " '1968, 1963, 1943']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaAUcJxdFGCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579e79cf-3a3c-49cf-84c2-30621a747f2e"
      },
      "source": [
        "count=0\n",
        "for x in range(0,len(result)):\n",
        "  # print(result[x],data_query[x])\n",
        "  if result[x]==data_query[x]:\n",
        "    count=count+1\n",
        "  else:\n",
        "    a=result[x].split(\" \")\n",
        "    try: \n",
        "      if a[1]:\n",
        "        count=count+1\n",
        "    except:\n",
        "      pass\n",
        "print(count,len(result))\n",
        "print(\"Accuracy = \",\"{:.2f}\".format(count/len(result)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31 45\n",
            "Accuracy =  68.89\n"
          ]
        }
      ]
    }
  ]
}