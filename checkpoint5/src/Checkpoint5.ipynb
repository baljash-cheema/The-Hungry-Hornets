{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Checkpoint5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKB8YaRk05Sl"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-research/tapas/blob/master/notebooks/sqa_predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5EACclxE7sP"
      },
      "source": [
        "Running a Tapas fine-tuned checkpoint\n",
        "---\n",
        "This notebook shows how to load and make predictions with TAPAS model, which was introduced in the paper: [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uI6zyIM20Kw4",
        "outputId": "45413266-a21a-4617-ce6b-add348c83477"
      },
      "source": [
        "! pip install tapas-table-parsing"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tapas-table-parsing\n",
            "  Downloading tapas_table_parsing-0.0.1.dev0-py3-none-any.whl (195 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▊                              | 10 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 30 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 195 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting tf-slim~=1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 38.7 MB/s \n",
            "\u001b[?25hCollecting tf-models-official~=2.2.0\n",
            "  Downloading tf_models_official-2.2.2-py2.py3-none-any.whl (711 kB)\n",
            "\u001b[K     |████████████████████████████████| 711 kB 33.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow-probability==0.10.1\n",
            "  Downloading tensorflow_probability-0.10.1-py2.py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 45.8 MB/s \n",
            "\u001b[?25hCollecting frozendict==1.2\n",
            "  Downloading frozendict-1.2.tar.gz (2.6 kB)\n",
            "Collecting pandas~=1.0.0\n",
            "  Downloading pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 33.1 MB/s \n",
            "\u001b[?25hCollecting kaggle<1.5.8\n",
            "  Downloading kaggle-1.5.6.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting scikit-learn~=0.22.1\n",
            "  Downloading scikit_learn-0.22.2.post1-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 20.8 MB/s \n",
            "\u001b[?25hCollecting nltk~=3.5\n",
            "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 41.9 MB/s \n",
            "\u001b[?25hCollecting tensorflow~=2.2.0\n",
            "  Downloading tensorflow-2.2.3-cp37-cp37m-manylinux2010_x86_64.whl (516.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 516.4 MB 17 kB/s \n",
            "\u001b[?25hCollecting apache-beam[gcp]==2.20.0\n",
            "  Downloading apache_beam-2.20.0-cp37-cp37m-manylinux1_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 35.6 MB/s \n",
            "\u001b[?25hCollecting oauth2client<4,>=2.0.1\n",
            "  Downloading oauth2client-3.0.0.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.5.0.post1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2,>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.42.0)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.7)\n",
            "Collecting mock<3.0.0,>=1.0.1\n",
            "  Downloading mock-2.0.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n",
            "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
            "Requirement already satisfied: future<1.0.0,>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (0.16.0)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (3.12.1)\n",
            "Collecting httplib2<=0.12.0,>=0.8\n",
            "  Downloading httplib2-0.12.0.tar.gz (218 kB)\n",
            "\u001b[K     |████████████████████████████████| 218 kB 44.0 MB/s \n",
            "\u001b[?25hCollecting typing-extensions<3.8.0,>=3.7.0\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (2.8.2)\n",
            "Collecting pyarrow<0.17.0,>=0.15.1\n",
            "  Downloading pyarrow-0.16.0-cp37-cp37m-manylinux2014_x86_64.whl (63.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 63.1 MB 38 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.19.5)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 50.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (2018.9)\n",
            "Collecting fastavro<0.22,>=0.21.4\n",
            "  Downloading fastavro-0.21.24-cp37-cp37m-manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 35.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.3.0)\n",
            "Collecting google-cloud-dlp<=0.13.0,>=0.12.0\n",
            "  Downloading google_cloud_dlp-0.13.0-py2.py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 52.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.0.3)\n",
            "Requirement already satisfied: google-cloud-bigquery<=1.24.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.21.0)\n",
            "Collecting google-cloud-vision<0.43.0,>=0.38.0\n",
            "  Downloading google_cloud_vision-0.42.0-py2.py3-none-any.whl (435 kB)\n",
            "\u001b[K     |████████████████████████████████| 435 kB 47.4 MB/s \n",
            "\u001b[?25hCollecting cachetools<4,>=3.1.0\n",
            "  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting grpcio-gcp<1,>=0.2.2\n",
            "  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
            "Collecting google-cloud-videointelligence<1.14.0,>=1.8.0\n",
            "  Downloading google_cloud_videointelligence-1.13.0-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 44.7 MB/s \n",
            "\u001b[?25hCollecting google-apitools<0.5.29,>=0.5.28\n",
            "  Downloading google-apitools-0.5.28.tar.gz (172 kB)\n",
            "\u001b[K     |████████████████████████████████| 172 kB 54.0 MB/s \n",
            "\u001b[?25hCollecting google-cloud-language<2,>=1.3.0\n",
            "  Downloading google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting google-cloud-datastore<1.8.0,>=1.7.1\n",
            "  Downloading google_cloud_datastore-1.7.4-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting google-cloud-spanner<1.14.0,>=1.13.0\n",
            "  Downloading google_cloud_spanner-1.13.0-py2.py3-none-any.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 46.0 MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigtable<1.1.0,>=0.31.1\n",
            "  Downloading google_cloud_bigtable-1.0.0-py2.py3-none-any.whl (232 kB)\n",
            "\u001b[K     |████████████████████████████████| 232 kB 52.4 MB/s \n",
            "\u001b[?25hCollecting google-cloud-pubsub<1.1.0,>=0.39.0\n",
            "  Downloading google_cloud_pubsub-1.0.2-py2.py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle==1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.10.1->tapas-table-parsing) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.10.1->tapas-table-parsing) (4.4.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.10.1->tapas-table-parsing) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.10.1->tapas-table-parsing) (0.4.0)\n",
            "Collecting fasteners>=0.14\n",
            "  Downloading fasteners-0.16.3-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<=1.24.0,>=1.6.0->apache-beam[gcp]==2.20.0->tapas-table-parsing) (0.4.1)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.26.3)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading grpc-google-iam-v1-0.12.3.tar.gz (13 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.53.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (1.35.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (2.23.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (21.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (4.8)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.20.0->tapas-table-parsing) (0.6.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from kaggle<1.5.8->tapas-table-parsing) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle<1.5.8->tapas-table-parsing) (2021.10.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle<1.5.8->tapas-table-parsing) (4.62.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle<1.5.8->tapas-table-parsing) (5.0.2)\n",
            "Collecting pbr>=0.11\n",
            "  Downloading pbr-5.8.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk~=3.5->tapas-table-parsing) (7.1.2)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2021.11.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 44.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk~=3.5->tapas-table-parsing) (1.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (0.4.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (3.0.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<1.1.0,>=0.31.1->apache-beam[gcp]==2.20.0->tapas-table-parsing) (2.10)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn~=0.22.1->tapas-table-parsing) (1.4.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (1.13.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (0.2.0)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 39.1 MB/s \n",
            "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 31.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (1.6.3)\n",
            "Collecting numpy<2,>=1.14.3\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |████████████████████████████████| 454 kB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (3.3.0)\n",
            "Collecting gast>=0.3.2\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2.0->tapas-table-parsing) (0.37.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (3.3.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (3.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0->tapas-table-parsing) (3.1.1)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 53.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (0.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (7.1.2)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (1.12.8)\n",
            "Collecting tensorflow-model-optimization>=0.2.1\n",
            "  Downloading tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 55.8 MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.6 MB 120 kB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting typing==3.7.4.1\n",
            "  Downloading typing-3.7.4.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (4.0.1)\n",
            "Collecting mlperf-compliance==0.0.10\n",
            "  Downloading mlperf_compliance-0.0.10-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (0.29.24)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (5.4.8)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (0.12.0)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (3.13)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.2.0->tapas-table-parsing) (3.2.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.2.0->tapas-table-parsing) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.2.0->tapas-table-parsing) (0.0.4)\n",
            "Collecting google-api-python-client>=1.6.7\n",
            "  Downloading google_api_python_client-2.31.0-py2.py3-none-any.whl (7.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8 MB 36.7 MB/s \n",
            "\u001b[?25hCollecting google-auth-httplib2>=0.1.0\n",
            "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Collecting google-api-python-client>=1.6.7\n",
            "  Downloading google_api_python_client-2.30.0-py2.py3-none-any.whl (7.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8 MB 9.8 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.29.0-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 38.8 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.28.0-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 23.9 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.27.0-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 20.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.26.1-py2.py3-none-any.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 25.4 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.26.0-py2.py3-none-any.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 18.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.25.0-py2.py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 27.5 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.24.0-py2.py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 33.0 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.23.0-py2.py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 16.7 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.22.0-py2.py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 24.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.21.0-py2.py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 16.5 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.20.0-py2.py3-none-any.whl (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 32.0 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.19.1-py2.py3-none-any.whl (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 53.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.19.0-py2.py3-none-any.whl (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 18.3 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.18.0-py2.py3-none-any.whl (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 18.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.17.0-py2.py3-none-any.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 35.9 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.15.0-py2.py3-none-any.whl (7.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2 MB 42.7 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.14.1-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 19.7 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.14.0-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 20.5 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.13.0-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 28.6 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.12.0-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 38.5 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.11.0-py2.py3-none-any.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 39.5 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.10.0-py2.py3-none-any.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 18.6 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.9.0-py2.py3-none-any.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 36.9 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.8.0-py2.py3-none-any.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 7.3 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.7.0-py2.py3-none-any.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 21.4 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.6.0-py2.py3-none-any.whl (7.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2 MB 19.4 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.5.0-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 16.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.4.0-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 15.3 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.3.0-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 43.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.2.0-py2.py3-none-any.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 16.5 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.1.0-py2.py3-none-any.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 25.5 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-2.0.2-py2.py3-none-any.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 36.3 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-1.12.7-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 21 kB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-1.12.6-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 24 kB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-1.12.5-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 8.0 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-1.12.4-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 7.8 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-1.12.3-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 7.5 MB/s \n",
            "\u001b[?25h  Downloading google_api_python_client-1.12.2-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.2.1->tf-models-official~=2.2.0->tapas-table-parsing) (0.1.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official~=2.2.0->tapas-table-parsing) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official~=2.2.0->tapas-table-parsing) (1.3.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle<1.5.8->tapas-table-parsing) (1.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official~=2.2.0->tapas-table-parsing) (2.7.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official~=2.2.0->tapas-table-parsing) (5.4.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official~=2.2.0->tapas-table-parsing) (2.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official~=2.2.0->tapas-table-parsing) (21.2.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official~=2.2.0->tapas-table-parsing) (1.4.0)\n",
            "Building wheels for collected packages: frozendict, avro-python3, dill, google-apitools, grpc-google-iam-v1, httplib2, kaggle, oauth2client, py-cpuinfo\n",
            "  Building wheel for frozendict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for frozendict: filename=frozendict-1.2-py3-none-any.whl size=3166 sha256=65d309a437c48c6c29d7943b385c2d3e8d6a5adc110ed1b6d8ae775fdf868359\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/17/69/ac196dd181e620bba5fae5488e4fd6366a7316dce13cf88776\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=5a86be7803f4f4624d2ec1fc33f4141ead9b1c92547495e790c170143d191998\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=47f31698ae509ab1757a16499067ecc34fae594aee190b16ee5d03f6296cb8fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.28-py3-none-any.whl size=130108 sha256=83d6b3f5f00e286ce15a8c15a933b45c89f772ed64cdddbd10632a491514fed4\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/3b/69/ecd8e6ae89d9d71102a58962c29faa7a9467ba45f99f205920\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-py3-none-any.whl size=18515 sha256=3e9520195c320ae94e5db7d3b0ee46c6ec3df285201afb7f9f582576ab9393a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/ee/67/2e444183030cb8d31ce8b34cee34a7afdbd3ba5959ea846380\n",
            "  Building wheel for httplib2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for httplib2: filename=httplib2-0.12.0-py3-none-any.whl size=93464 sha256=0fff41ed3f932152c24b6cd233348cbd9380c306458de7e3a3f14948b50f37a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/e7/b6/0dd30343ceca921cfbd91f355041bd9c69e0f40b49f25b7b8a\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.6-py3-none-any.whl size=72857 sha256=df7607c2d1bd5135c8250fa9b7adfe442e561171a708a296e03ab4fe59dbe8bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/e7/e7/eb3c3d514c33294d77ddd5a856bdd58dc9c1fabbed59a02a2b\n",
            "  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oauth2client: filename=oauth2client-3.0.0-py3-none-any.whl size=106375 sha256=dd27b62bd9262a3c2d6468e1d47fc58279a326bd4ce6d14a403e909f58469d8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/73/7a/3b3f76a2142176605ff38fbca574327962c71e25a43197a4c1\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22258 sha256=d7b2a240bf6ceb4d337c4328a789d05e9630556335cddbf5c74c3b891fb27553\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "Successfully built frozendict avro-python3 dill google-apitools grpc-google-iam-v1 httplib2 kaggle oauth2client py-cpuinfo\n",
            "Installing collected packages: typing-extensions, cachetools, pbr, numpy, httplib2, grpcio-gcp, tensorflow-estimator, tensorboard, pyarrow, oauth2client, mock, hdfs, h5py, grpc-google-iam-v1, gast, fasteners, fastavro, dill, avro-python3, typing, tensorflow-model-optimization, tensorflow-addons, tensorflow, sentencepiece, regex, py-cpuinfo, pandas, opencv-python-headless, mlperf-compliance, kaggle, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-apitools, google-api-python-client, dataclasses, apache-beam, tf-slim, tf-models-official, tensorflow-probability, scikit-learn, nltk, frozendict, tapas-table-parsing\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 4.2.4\n",
            "    Uninstalling cachetools-4.2.4:\n",
            "      Successfully uninstalled cachetools-4.2.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: httplib2\n",
            "    Found existing installation: httplib2 0.17.4\n",
            "    Uninstalling httplib2-0.17.4:\n",
            "      Successfully uninstalled httplib2-0.17.4\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "  Attempting uninstall: oauth2client\n",
            "    Found existing installation: oauth2client 4.1.3\n",
            "    Uninstalling oauth2client-4.1.3:\n",
            "      Successfully uninstalled oauth2client-4.1.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "  Attempting uninstall: google-cloud-language\n",
            "    Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "  Attempting uninstall: google-cloud-datastore\n",
            "    Found existing installation: google-cloud-datastore 1.8.0\n",
            "    Uninstalling google-cloud-datastore-1.8.0:\n",
            "      Successfully uninstalled google-cloud-datastore-1.8.0\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.15.0\n",
            "    Uninstalling tensorflow-probability-0.15.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.15.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc3 3.11.4 requires cachetools>=4.2.1, but you have cachetools 3.1.1 which is incompatible.\n",
            "pydrive 1.3.1 requires oauth2client>=4.0.0, but you have oauth2client 3.0.0 which is incompatible.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.0.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed apache-beam-2.20.0 avro-python3-1.9.2.1 cachetools-3.1.1 dataclasses-0.6 dill-0.3.1.1 fastavro-0.21.24 fasteners-0.16.3 frozendict-1.2 gast-0.3.3 google-api-python-client-1.12.2 google-apitools-0.5.28 google-cloud-bigtable-1.0.0 google-cloud-datastore-1.7.4 google-cloud-dlp-0.13.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.0.2 google-cloud-spanner-1.13.0 google-cloud-videointelligence-1.13.0 google-cloud-vision-0.42.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 h5py-2.10.0 hdfs-2.6.0 httplib2-0.12.0 kaggle-1.5.6 mlperf-compliance-0.0.10 mock-2.0.0 nltk-3.6.5 numpy-1.18.5 oauth2client-3.0.0 opencv-python-headless-4.5.4.60 pandas-1.0.5 pbr-5.8.0 py-cpuinfo-8.0.0 pyarrow-0.16.0 regex-2021.11.10 scikit-learn-0.22.2.post1 sentencepiece-0.1.96 tapas-table-parsing-0.0.1.dev0 tensorboard-2.2.2 tensorflow-2.2.3 tensorflow-addons-0.15.0 tensorflow-estimator-2.2.0 tensorflow-model-optimization-0.7.0 tensorflow-probability-0.10.1 tf-models-official-2.2.2 tf-slim-1.1.0 typing-3.7.4.1 typing-extensions-3.7.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy",
                  "pandas",
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7We9ofHuFMuk"
      },
      "source": [
        "# Fetch models fom Google Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA1jUByqyUNB"
      },
      "source": [
        "Next we can get pretrained checkpoint from Google Storage. For the sake of speed, this is base sized model trained on [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253). Note that best results in the paper were obtained with a large model, with 24 layers instead of 12."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B10C0Yz6gQyD",
        "outputId": "762e62a5-a01a-43e4-84b7-5c8b2af53515"
      },
      "source": [
        "\n",
        "! gsutil cp gs://tapas_models/2020_04_21/tapas_sqa_base.zip . && unzip tapas_sqa_base.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://tapas_models/2020_04_21/tapas_sqa_base.zip...\n",
            "/ [1 files][  1.0 GiB/  1.0 GiB]   44.7 MiB/s                                   \n",
            "Operation completed over 1 objects/1.0 GiB.                                      \n",
            "Archive:  tapas_sqa_base.zip\n",
            "   creating: tapas_sqa_base/\n",
            "  inflating: tapas_sqa_base/model.ckpt.data-00000-of-00001  \n",
            "  inflating: tapas_sqa_base/model.ckpt.index  \n",
            "  inflating: tapas_sqa_base/README.txt  \n",
            "  inflating: tapas_sqa_base/vocab.txt  \n",
            "  inflating: tapas_sqa_base/bert_config.json  \n",
            "  inflating: tapas_sqa_base/model.ckpt.meta  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3107bGlGm7d"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnUjDlLqDd3m"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import os \n",
        "import shutil\n",
        "import csv\n",
        "import pandas as pd\n",
        "import IPython\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aml6oLFl1dSt"
      },
      "source": [
        "from tapas.utils import tf_example_utils\n",
        "from tapas.protos import interaction_pb2\n",
        "from tapas.utils import number_annotation_utils\n",
        "from tapas.scripts import prediction_utils"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbMUYT1bKMp9"
      },
      "source": [
        "# Load checkpoint for prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO0d_wFMy82O"
      },
      "source": [
        "Here's the prediction code, which will create and `interaction_pb2.Interaction` protobuf object, which is the datastructure we use to store examples, and then call the prediction script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHJqO-pyxwTD",
        "outputId": "4d3ec98f-dd9d-4b36-952c-296b1da53a68"
      },
      "source": [
        "import psycopg2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJKuRn_wxtns"
      },
      "source": [
        "# access the postgresql server\n",
        "conn = psycopg2.connect(\n",
        "    host=\"codd04.research.northwestern.edu\",\n",
        "    port = \"5433\",\n",
        "    database=\"postgres\",\n",
        "    user=\"cpdbstudent\",\n",
        "    password=\"DataSci4AI\")\n",
        "cursor = conn.cursor()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk4-kzU7x9s7"
      },
      "source": [
        "edges_query = '''\n",
        "DROP TABLE IF EXISTS da_category_ids;\n",
        "CREATE TEMP TABLE da_category_ids AS (\n",
        "(SELECT data_officerallegation.id,data_allegationcategory.allegation_name,data_allegationcategory.category\n",
        "FROM data_officerallegation\n",
        "join data_allegationcategory on data_officerallegation.allegation_category_id = data_allegationcategory.id\n",
        "WHERE data_allegationcategory.category = 'Drug / Alcohol Abuse' OR data_allegationcategory.category = 'Medical' or allegation_name LIKE 'Medical Roll%'\n",
        "OR data_allegationcategory.category_code IN ('024', '003', '003A', '003B', '003C', '003D', '003E')));\n",
        "\n",
        "SELECT gender,race,birth_year,first_name || ' ' || last_name officer_name,allegation_count,sustained_count,current_salary\n",
        "FROM data_officer\n",
        "join da_category_ids on data_officer.id=da_category_ids.id\n",
        "where data_officer.current_salary IS NOT NULL and allegation_count>2 and sustained_count>1 order by officer_name desc;\n",
        "'''"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL5FkrmJyI3P",
        "outputId": "265f0f07-9591-462d-fbb1-b3654956348f"
      },
      "source": [
        "cursor.execute(edges_query)\n",
        "colnames = [desc[0] for desc in cursor.description]\n",
        "edges = cursor.fetchall()\n",
        "res = [[str(x) for x in list(ele)] for ele in edges]\n",
        "res.insert(0, colnames)\n",
        "print(res)\n",
        "b=[el[6] for el in res][1:]\n",
        "print(b)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['gender', 'race', 'birth_year', 'officer_name', 'allegation_count', 'sustained_count', 'current_salary'], ['M', 'Hispanic', '1952', 'Wilman Dones', '23', '6', '82008'], ['M', 'Black', '1963', 'Vernard Ross', '23', '3', '107988'], ['M', 'White', '1957', 'Thomas Motzny', '31', '2', '102978'], ['M', 'White', '1949', 'Thomas Biggane', '30', '2', '82878'], ['M', 'White', '1969', 'Steven Nowicki', '27', '3', '111474'], ['M', 'White', '1970', 'Steven Bechina', '43', '4', '107988'], ['M', 'White', '1948', 'Ronald Blake', '6', '2', '83604'], ['M', 'Black', '1948', 'Rollins Johnson', '6', '2', '88260'], ['M', 'Hispanic', '1985', 'Roger Farias', '14', '2', '84054'], ['M', 'Black', '1964', 'Ricky Bean', '11', '3', '100980'], ['M', 'White', '1961', 'Raymond Gadomski', '17', '3', '96060'], ['M', 'Black', '1953', 'Prentiss Jackson', '15', '5', '102978'], ['M', 'White', '1962', 'Philip Paluch', '56', '2', '111474'], ['F', 'Black', '1950', 'Paularie Draine', '7', '2', '78012'], ['F', 'Black', '1948', 'Patricia Ballentine', '16', '6', '79926'], ['M', 'Hispanic', '1971', 'Orlando Fonseca', '7', '2', '83616'], ['M', 'White', '1955', 'Nicholas Dimaggio', '23', '15', '87888'], ['M', 'Black', '1951', 'Michael Overstreet', '56', '10', '78006'], ['M', 'Black', '1973', 'Marvin Randolph', '14', '3', '90024'], ['M', 'White', '1947', 'Marshall Pufundt', '23', '2', '58884'], ['M', 'Black', '1959', 'Marco Johnson', '24', '6', '86130'], ['M', 'Hispanic', '1967', 'Luis Lopez', '8', '3', '93354'], ['F', 'Black', '1961', 'Lisa William-Handley', '13', '2', '90618'], ['F', 'Black', '1963', 'Linda Brumfield', '17', '8', '68262'], ['M', 'Black', '1956', 'Larry Dotson', '43', '3', '90540'], ['F', 'Black', '1970', 'Lakisa Anderson', '23', '4', '93354'], ['F', 'Black', '1964', 'Kimberly Hill', '24', '3', '86130'], ['M', 'White', '1960', 'Kevin Keyes', '32', '11', '69270'], ['M', 'White', '1979', 'Joseph Thompson', '9', '3', '101442'], ['M', 'White', '1961', 'Joseph Battaglia', '66', '4', '83706'], ['M', 'Black', '1957', 'John Brownridge', '54', '4', '100980'], ['M', 'Hispanic', '1959', 'Jesus Avila', '22', '4', '95106'], ['F', 'Black', '1965', 'Jeanetta Brown Cunningha', '5', '4', '99888'], ['F', 'White', '1981', 'Jacklyn Mueller', '27', '2', '87006'], ['M', 'Hispanic', '1965', 'Irwin Negron', '33', '5', '96060'], ['M', 'Black', '1968', 'Haki Akintunde', '12', '4', '70656'], ['M', 'Black', '1968', 'Frederick Anthony', '29', '2', '92430'], ['M', 'White', '1963', 'Emil Bux', '13', '3', '93354'], ['M', 'Black', '1959', 'Donald Banks', '10', '2', '92316'], ['M', 'White', '1954', 'Dennis Peca', '11', '2', '90456'], ['M', 'Black', '1970', 'Brian Burton', '42', '3', '93354'], ['M', 'Black', '1962', 'Anthony Singleton', '5', '2', '90024'], ['F', 'Black', '1943', 'Anita Ashton', '23', '4', '71682'], ['M', 'Black', '1949', 'Alvin Ward', '11', '5', '69264'], ['M', 'Black', '1963', 'Alvin Greenup', '27', '3', '93354']]\n",
            "['82008', '107988', '102978', '82878', '111474', '107988', '83604', '88260', '84054', '100980', '96060', '102978', '111474', '78012', '79926', '83616', '87888', '78006', '90024', '58884', '86130', '93354', '90618', '68262', '90540', '93354', '86130', '69270', '101442', '83706', '100980', '95106', '99888', '87006', '96060', '70656', '92430', '93354', '92316', '90456', '93354', '90024', '71682', '69264', '93354']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKfxspnVFPsc"
      },
      "source": [
        "os.makedirs('results/sqa/tf_examples', exist_ok=True)\n",
        "os.makedirs('results/sqa/model', exist_ok=True)\n",
        "with open('results/sqa/model/checkpoint', 'w') as f:\n",
        "  f.write('model_checkpoint_path: \"model.ckpt-0\"')\n",
        "for suffix in ['.data-00000-of-00001', '.index', '.meta']:\n",
        "  shutil.copyfile(f'tapas_sqa_base/model.ckpt{suffix}', f'results/sqa/model/model.ckpt-0{suffix}')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RlvgDAmCNtP"
      },
      "source": [
        "max_seq_length = 512\n",
        "vocab_file = \"tapas_sqa_base/vocab.txt\"\n",
        "config = tf_example_utils.ClassifierConversionConfig(\n",
        "    vocab_file=vocab_file,\n",
        "    max_seq_length=max_seq_length,\n",
        "    max_column_id=max_seq_length,\n",
        "    max_row_id=max_seq_length,\n",
        "    strip_column_names=False,\n",
        "    add_aggregation_candidates=False,\n",
        ")\n",
        "converter = tf_example_utils.ToClassifierTensorflowExample(config)\n",
        "\n",
        "def convert_interactions_to_examples(tables_and_queries):\n",
        "  \"\"\"Calls Tapas converter to convert interaction to example.\"\"\"\n",
        "  for idx, (table, queries) in enumerate(tables_and_queries):\n",
        "    interaction = interaction_pb2.Interaction()\n",
        "    for position, query in enumerate(queries):\n",
        "      question = interaction.questions.add()\n",
        "      question.original_text = query\n",
        "      question.id = f\"{idx}-0_{position}\"\n",
        "    for header in table[0]:\n",
        "      interaction.table.columns.add().text = header\n",
        "    for line in table[1:]:\n",
        "      row = interaction.table.rows.add()\n",
        "      for cell in line:\n",
        "        row.cells.add().text = cell\n",
        "    number_annotation_utils.add_numeric_values(interaction)\n",
        "    for i in range(len(interaction.questions)):\n",
        "      try:\n",
        "        yield converter.convert(interaction, i)\n",
        "      except ValueError as e:\n",
        "        print(f\"Can't convert interaction: {interaction.id} error: {e}\")\n",
        "        \n",
        "def write_tf_example(filename, examples):\n",
        "  with tf.io.TFRecordWriter(filename) as writer:\n",
        "    for example in examples:\n",
        "      writer.write(example.SerializeToString())\n",
        "\n",
        "def predict(table_data, queries):\n",
        "  answers=[]\n",
        "  table = res\n",
        "  examples = convert_interactions_to_examples([(table, queries)])\n",
        "  write_tf_example(\"results/sqa/tf_examples/test.tfrecord\", examples)\n",
        "  write_tf_example(\"results/sqa/tf_examples/random-split-1-dev.tfrecord\", [])\n",
        "  \n",
        "  ! python -m tapas.run_task_main \\\n",
        "    --task=\"SQA\" \\\n",
        "    --output_dir=\"results\" \\\n",
        "    --noloop_predict \\\n",
        "    --test_batch_size={len(queries)} \\\n",
        "    --tapas_verbosity=\"ERROR\" \\\n",
        "    --compression_type= \\\n",
        "    --init_checkpoint=\"tapas_sqa_base/model.ckpt\" \\\n",
        "    --bert_config_file=\"tapas_sqa_base/bert_config.json\" \\\n",
        "    --mode=\"predict\" 2> error\n",
        "\n",
        "\n",
        "  results_path = \"results/sqa/model/test_sequence.tsv\"\n",
        "  all_coordinates = []\n",
        "  df = pd.DataFrame(table[1:], columns=table[0])\n",
        "  with open(results_path) as csvfile:\n",
        "    reader = csv.DictReader(csvfile, delimiter='\\t')\n",
        "    for row in reader:\n",
        "      coordinates = prediction_utils.parse_coordinates(row[\"answer_coordinates\"])\n",
        "      all_coordinates.append(coordinates)\n",
        "      answers.append(', '.join([table[row + 1][col] for row, col in coordinates]))\n",
        "      position = int(row['position'])\n",
        "      print(\">\", queries[position])\n",
        "      print(answers)\n",
        "  return answers"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqu-I-M9QaoA"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIE7bTJMVuSh",
        "outputId": "280729cd-b18c-42a5-cd80-f94566a0a86b"
      },
      "source": [
        "# Example nu-1000-0\n",
        "result = predict('', [\"what is the current salary of Wilman Dones?\",\"what is the current salary of Vernard Ross?\",\"what is the current salary of Thomas Motzny?\",\"what is the current salary of Thomas Biggane?\",\"what is the current salary of Steven Nowicki?\",\"what is the current salary of Steven Bechina?\",\"what is the current salary of Ronald Blake?\",\"what is the current salary of Rollins Johnson?\",\"what is the current salary of Roger Farias?\",\"what is the current salary of Ricky Bean?\",\"what is the current salary of Raymond Gadomski?\",\"what is the current salary of Prentiss Jackson?\",\"what is the current salary of Philip Paluch?\",\"what is the current salary of Paularie Draine?\",\"what is the current salary of Patricia Ballentine?\",\"what is the current salary of Orlando Fonseca?\",\"what is the current salary of Nicholas Dimaggio?\",\"what is the current salary of Michael Overstreet?\",\"what is the current salary of Marvin Randolph?\",\"what is the current salary of Marshall Pufundt?\",\"what is the current salary of Marco Johnson?\",\"what is the current salary of Luis Lopez?\",\"what is the current salary of Lisa William-Handley?\",\"what is the current salary of Linda Brumfield?\",\"what is the current salary of Larry Dotson?\",\"what is the current salary of Lakisa Anderson?\",\"what is the current salary of Kimberly Hill?\",\"what is the current salary of Kevin Keyes?\",\"what is the current salary of Joseph Thompson?\",\"what is the current salary of Joseph Battaglia?\",\"what is the current salary of John Brownridge?\",\"what is the current salary of Jesus Avila?\",\"what is the current salary of Jeanetta Brown Cunningha?\",\"what is the current salary of Jacklyn Mueller?\",\"what is the current salary of Irwin Negron?\",\"what is the current salary of Haki Akintunde?\",\"what is the current salary of Frederick Anthony?\",\"what is the current salary of Emil Bux?\",\"what is the current salary of Donald Banks?\",\"what is the current salary of Dennis Peca?\",\"what is the current salary of Brian Burton?\",\"what is the current salary of Anthony Singleton?\",\"what is the current salary of Anita Ashton?\",\"what is the current salary of Alvin Ward?\",\"what is the current salary of Alvin Greenup?\"])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is_built_with_cuda: True\n",
            "is_gpu_available: True\n",
            "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Training or predicting ...\n",
            "Evaluation finished after training step 0.\n",
            "> what is the current salary of Wilman Dones?\n",
            "['82008']\n",
            "> what is the current salary of Vernard Ross?\n",
            "['82008', '83706']\n",
            "> what is the current salary of Thomas Motzny?\n",
            "['82008', '83706', '82878']\n",
            "> what is the current salary of Thomas Biggane?\n",
            "['82008', '83706', '82878', '']\n",
            "> what is the current salary of Steven Nowicki?\n",
            "['82008', '83706', '82878', '', '107988']\n",
            "> what is the current salary of Steven Bechina?\n",
            "['82008', '83706', '82878', '', '107988', '']\n",
            "> what is the current salary of Ronald Blake?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604']\n",
            "> what is the current salary of Rollins Johnson?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '']\n",
            "> what is the current salary of Roger Farias?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054']\n",
            "> what is the current salary of Ricky Bean?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980']\n",
            "> what is the current salary of Raymond Gadomski?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988']\n",
            "> what is the current salary of Prentiss Jackson?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978']\n",
            "> what is the current salary of Philip Paluch?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '']\n",
            "> what is the current salary of Paularie Draine?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012']\n",
            "> what is the current salary of Patricia Ballentine?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926']\n",
            "> what is the current salary of Orlando Fonseca?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262']\n",
            "> what is the current salary of Nicholas Dimaggio?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888']\n",
            "> what is the current salary of Michael Overstreet?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006']\n",
            "> what is the current salary of Marvin Randolph?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '']\n",
            "> what is the current salary of Marshall Pufundt?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '']\n",
            "> what is the current salary of Marco Johnson?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '']\n",
            "> what is the current salary of Luis Lopez?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '']\n",
            "> what is the current salary of Lisa William-Handley?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '']\n",
            "> what is the current salary of Linda Brumfield?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262']\n",
            "> what is the current salary of Larry Dotson?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540']\n",
            "> what is the current salary of Lakisa Anderson?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354']\n",
            "> what is the current salary of Kimberly Hill?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980']\n",
            "> what is the current salary of Kevin Keyes?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '']\n",
            "> what is the current salary of Joseph Thompson?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706']\n",
            "> what is the current salary of Joseph Battaglia?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '']\n",
            "> what is the current salary of John Brownridge?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '']\n",
            "> what is the current salary of Jesus Avila?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '']\n",
            "> what is the current salary of Jeanetta Brown Cunningha?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '', '99888']\n",
            "> what is the current salary of Jacklyn Mueller?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '', '99888', '87006']\n",
            "> what is the current salary of Irwin Negron?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '', '99888', '87006', '']\n",
            "> what is the current salary of Haki Akintunde?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '', '99888', '87006', '', '70656']\n",
            "> what is the current salary of Frederick Anthony?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '', '99888', '87006', '', '70656', '92430']\n",
            "> what is the current salary of Emil Bux?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '', '99888', '87006', '', '70656', '92430', '93354']\n",
            "> what is the current salary of Donald Banks?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '', '99888', '87006', '', '70656', '92430', '93354', '92316']\n",
            "> what is the current salary of Dennis Peca?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '', '99888', '87006', '', '70656', '92430', '93354', '92316', '']\n",
            "> what is the current salary of Brian Burton?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '', '99888', '87006', '', '70656', '92430', '93354', '92316', '', '93354']\n",
            "> what is the current salary of Anthony Singleton?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '', '99888', '87006', '', '70656', '92430', '93354', '92316', '', '93354', '']\n",
            "> what is the current salary of Anita Ashton?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '', '99888', '87006', '', '70656', '92430', '93354', '92316', '', '93354', '', '71682']\n",
            "> what is the current salary of Alvin Ward?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '', '99888', '87006', '', '70656', '92430', '93354', '92316', '', '93354', '', '71682', '93354']\n",
            "> what is the current salary of Alvin Greenup?\n",
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '', '99888', '87006', '', '70656', '92430', '93354', '92316', '', '93354', '', '71682', '93354', '93354']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie1K13oXYZch",
        "outputId": "9ee4fa65-52a6-40a5-e04b-1f3bc075cc6a"
      },
      "source": [
        "print(result)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['82008', '83706', '82878', '', '107988', '', '83604', '', '84054', '100980', '96060, 107988', '102978', '', '78012', '79926', '68262', '87888', '78006', '', '', '', '', '', '68262', '90540', '93354', '86130, 100980', '', '83706', '', '', '', '99888', '87006', '', '70656', '92430', '93354', '92316', '', '93354', '', '71682', '93354', '93354']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENPSwio_Sf2g",
        "outputId": "f9bd6f84-7082-4379-d847-1295530670e9"
      },
      "source": [
        "b"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['82008',\n",
              " '107988',\n",
              " '102978',\n",
              " '82878',\n",
              " '111474',\n",
              " '107988',\n",
              " '83604',\n",
              " '88260',\n",
              " '84054',\n",
              " '100980',\n",
              " '96060',\n",
              " '102978',\n",
              " '111474',\n",
              " '78012',\n",
              " '79926',\n",
              " '83616',\n",
              " '87888',\n",
              " '78006',\n",
              " '90024',\n",
              " '58884',\n",
              " '86130',\n",
              " '93354',\n",
              " '90618',\n",
              " '68262',\n",
              " '90540',\n",
              " '93354',\n",
              " '86130',\n",
              " '69270',\n",
              " '101442',\n",
              " '83706',\n",
              " '100980',\n",
              " '95106',\n",
              " '99888',\n",
              " '87006',\n",
              " '96060',\n",
              " '70656',\n",
              " '92430',\n",
              " '93354',\n",
              " '92316',\n",
              " '90456',\n",
              " '93354',\n",
              " '90024',\n",
              " '71682',\n",
              " '69264',\n",
              " '93354']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5myz3j_JTEoa",
        "outputId": "2bd254ca-ac0a-43d7-d75a-5cc167dded67"
      },
      "source": [
        "count=0\n",
        "for x in result:\n",
        "  for y in b:\n",
        "    if x==y:\n",
        "      print(x,y)\n",
        "      count=count+1\n",
        "print(count)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82008 82008\n",
            "83706 83706\n",
            "82878 82878\n",
            "107988 107988\n",
            "107988 107988\n",
            "83604 83604\n",
            "84054 84054\n",
            "100980 100980\n",
            "100980 100980\n",
            "102978 102978\n",
            "102978 102978\n",
            "78012 78012\n",
            "79926 79926\n",
            "68262 68262\n",
            "87888 87888\n",
            "78006 78006\n",
            "68262 68262\n",
            "90540 90540\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "83706 83706\n",
            "99888 99888\n",
            "87006 87006\n",
            "70656 70656\n",
            "92430 92430\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "92316 92316\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "71682 71682\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "93354 93354\n",
            "50\n"
          ]
        }
      ]
    }
  ]
}